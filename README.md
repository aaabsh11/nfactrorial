AI ChatBot

Описание проекта

Проект "AI ChatBot" представляет собой веб-приложение для общения с искусственным интеллектом. Фронтенд реализован на Vue 3 с использованием Vuetify и Pinia, бэкенд — на Express.js. В качестве языковых моделей используется комбинация DialoGPT-small (117 МБ) для базового диалога и Flan-T5-small (≈80 МБ) в роли фоллбэка для более развёрнутых ответов через бесплатный Hugging Face Inference API.

Установка и запуск

Клонировать репозиторий:

git clone https://github.com/AAABSH11/ai-chatbot.git
cd ai-chatbot

Установка и запуск бэкенда:

cd server
npm install
# Создать файл .env с:
# HF_TOKEN=hf_ВАШ_ТОКЕН
npm start

Установка и запуск фронтенда:

cd ../client
npm install
npm run dev        # локальная разработка
npm run build      # продакшн-сборка
npm run deploy     # деплой на GitHub Pages

Процесс проектирования и разработки

Начали с постановки задачи: простой UI для отправки сообщений и отображения ответов ИИ.

Выбрали Vue 3 и Vuetify для быстрого создания Material Design интерфейса.

Pinia использовали для хранения состояния темы (светлая/тёмная).

Express.js выбран за лёгкость настройки API и централизованное управление внешними сервисами.

Hugging Face Inference API интегрирован на сервере через Axios для защиты токена.

Уникальные подходы и методологии

Двойная модель: используем DialoGPT-small для быстрого и лёгкого базового чата и Flan-T5-small для более развернутых ответов, если первая модель недоступна.

Retry и x-wait-for-model: при редких 503-ошибках автоматически повторяем запрос и используем заголовок x-wait-for-model: true для ожидания загрузки модели, а не мгновенного отказа.

Один язык в стеке (JavaScript): как новичок, упростил разработку, выбрав один язык для фронтенда и бэкенда.

Компромиссы

Выбор DialoGPT-small и Flan-T5-small обеспечивает бесплатность и стабильность работы, но отвечает менее глубоко, чем крупные LLM, такие как Llama 2 или GPT-3.5.

Отказ от более сложных моделей с RLHF обусловлен лимитами бесплатного API и частыми 503.

Отсутствие сохранения полной истории диалога на сервере означает, что ответы могут быть менее контекстными.

Известные ошибки и проблемы

Нечеловечность ответов: модель часто отвечает кратко и механически.

Отсутствие контекста беседы: без передачи истории диалога ответы могут быть несвязными.

Зависимость от внешнего API: при проблемах HF Inference API возможны задержки или ошибки 503.

Выбор технологического стека

Vue 3 + Vuetify: быстрый и знакомый Material Design; активно поддерживается.

Pinia: современная замена Vuex с простым API для глобального состояния.

Express.js: минималистичный фреймворк для сервера, удобное подключение middleware и маршрутов.

Axios: простота HTTP-запросов и поддержка Node/Vue.

Hugging Face Inference API: бесплатный доступ к публичным LLM без развёртывания моделей.

JavaScript (Node.js + Vue 3): так как я новичок, использование одного языка для бэкенда и фронтенда упрощает разработку и поддержку приложения.
